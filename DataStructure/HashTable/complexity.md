# Complexity
Algorithm		Average	Worst case  
Search		O(1)	O(n)  
Insert		O(1)	O(n)  
Delete		O(1)	O(n)  

最悪ケースがO(n)なのはなぜか？  
キーを元にハッシュ関数の結果で値を格納する配列の添字を割り出す際、  
添字が他のキーと被ってしまう際にO(n)になり得る。  
被った時の対応方式は大きく2パターンで、オープンアドレスとチェイン方式がある。  

- オープンアドレス
```
被ったらキーに+1してハッシュ関数し直して、空いてる添字にたどり着くまでそれを繰り返す
```
- チェイン方式
```
被ったら前方連結リストのように被ったキーを繋いでいく
```

チェイン方式がメジャーっぽいのでそっちの場合を考える。  
と、格納してるキーが全て同じ添字の場合、その添字内のforward_listを線形探索するのにO(n)がかかってしまう、というのが最悪ケースO(n)の正体っぽい  

- std::vector<std::forward_list<T>>
と考えると分かりやすい模様  

実際はハッシュ関数は賢く実装されてる模様なのでO(n)は考えずO(1)とみなして支障ないらしい
